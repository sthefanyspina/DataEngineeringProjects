# ğŸš€ PySpark Streaming com Kafka

Este projeto demonstra um pipeline **de streaming em tempo real** usando **PySpark Structured Streaming + Kafka**.

## ğŸ“‚ Estrutura
pyspark_streaming_kafka/
â”‚
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ producer.py # Gera eventos e envia para o Kafka
â”‚ â””â”€â”€ consumer_stream.py # LÃª, processa e exibe resultados com PySpark
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

markdown
Copy code

## âš™ï¸ Requisitos
- Python 3.8+
- Apache Kafka (rodando localmente)
- PySpark 3.5+
- kafka-python

Instale as dependÃªncias:
```bash
pip install -r requirements.txt
â–¶ï¸ Como Rodar
Suba o Kafka local:

bash
Copy code
zookeeper-server-start.sh config/zookeeper.properties
kafka-server-start.sh config/server.properties
kafka-topics.sh --create --topic pedidos_topic --bootstrap-server localhost:9092
Rode o Producer:

bash
Copy code
python app/producer.py
Rode o Streaming:

bash
Copy code
spark-submit app/consumer_stream.py
ğŸ“Š Resultado
O PySpark exibirÃ¡, em tempo real, o total de pedidos por status no console.

lua
Copy code
+----------+---------------+
| status   | total_pedidos |
+----------+---------------+
| pago     | 15            |
| pendente | 7             |
| cancelado| 3             |
+----------+---------------+
ğŸ§© ExtensÃµes possÃ­veis
Escrever a saÃ­da em PostgreSQL ou Delta Lake

Criar janelas de tempo (window) para agregaÃ§Ãµes por minuto

Enviar alertas em tempo real (ex: volume alto de pedidos cancelados)

ğŸ“ Autor: Sthefany Spina
ğŸ’¡ Tecnologias: PySpark, Kafka, Structured Streaming