# ğŸ§¹ Pipeline de NormalizaÃ§Ã£o de Dados com PySpark

Projeto completo de **limpeza, padronizaÃ§Ã£o e validaÃ§Ã£o de dados** usando **Apache Spark (PySpark)**.  
O pipeline lÃª um dataset bruto de clientes, normaliza campos (nome, CPF, data e telefone), valida formatos e separa registros **vÃ¡lidos** e **rejeitados** em arquivos otimizados.

---

## âš™ï¸ **Funcionalidades**

âœ… PadronizaÃ§Ã£o de campos:
- **Nome:** remoÃ§Ã£o de espaÃ§os e formataÃ§Ã£o em maiÃºsculas  
- **CPF:** limpeza de caracteres e validaÃ§Ã£o por dÃ­gito verificador  
- **Data de Nascimento:** conversÃ£o para formato `yyyy-MM-dd`  
- **Telefone:** limpeza de sÃ­mbolos e validaÃ§Ã£o de tamanho  

âœ… ValidaÃ§Ã£o avanÃ§ada:
- CPF real (com dÃ­gito verificador)
- Telefone com 10 ou 11 dÃ­gitos
- Nomes nÃ£o vazios
- Datas vÃ¡lidas

âœ… SeparaÃ§Ã£o automÃ¡tica:
- **dados_limpios.parquet** â†’ registros validados e limpos  
- **dados_rejeitados.csv** â†’ registros com erros + motivo(s) de rejeiÃ§Ã£o  

âœ… Registro de logs:
- Logs detalhados em `logs/pipeline_normalizacao_avancado.log`

---

## ğŸ§± **Estrutura do Projeto**

pipeline_normalizacao/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/
â”‚ â”‚ â””â”€â”€ clientes_com_invalidos.csv # dataset bruto (205.000 linhas)
â”‚ â”œâ”€â”€ processed/
â”‚ â”‚ â”œâ”€â”€ dados_limpios.parquet # dados limpos
â”‚ â”‚ â””â”€â”€ dados_rejeitados.csv # registros rejeitados
â”‚
â”œâ”€â”€ logs/
â”‚ â””â”€â”€ pipeline_normalizacao_avancado.log
â”‚
â”œâ”€â”€ main_normalizacao_avancado.py
â”œâ”€â”€ README.md

yaml
Copy code

---

## ğŸ§  **Exemplo de Entrada**

`clientes_com_invalidos.csv`
```csv
nome,cpf,data_nascimento,telefone
JoÃ£o da Silva,111.222.333-44,01/05/1990,(11) 99999-8888
Maria Souza,22233344455,10/12/1985,21 88888-7777
,33344455566,15/03/1978,9999
ğŸ“Š Exemplo de SaÃ­da
ğŸŸ¢ dados_limpios.parquet
nome	cpf	data_nascimento	telefone
JOÃƒO DA SILVA	11122233344	1990-05-01	11999998888
MARIA SOUZA	22233344455	1985-12-10	21888887777

ğŸ”´ dados_rejeitados.csv
nome	cpf	data_nascimento	telefone	motivo_rejeicao
33344455566	15/03/1978	9999	Nome vazio, Telefone invÃ¡lido

ğŸš€ Como Executar o Pipeline
1ï¸âƒ£ Criar ambiente
bash
Copy code
python -m venv venv
source venv/bin/activate   # ou venv\Scripts\activate no Windows
2ï¸âƒ£ Instalar dependÃªncias
bash
Copy code
pip install pyspark faker pandas
3ï¸âƒ£ Rodar o script
bash
Copy code
python main_normalizacao_avancado.py
ğŸ§© Dataset
Um dataset sintÃ©tico com 205.000 registros foi gerado com a biblioteca Faker para testes:

ğŸ“„ clientes_com_invalidos.csv

200.000 registros vÃ¡lidos

5.000 registros invÃ¡lidos para testar rejeiÃ§Ãµes

ğŸ§° Tecnologias Utilizadas
Tecnologia	FunÃ§Ã£o
Apache Spark (PySpark)	Processamento distribuÃ­do
Python 3.x	Linguagem principal
Pandas + Faker	GeraÃ§Ã£o de dataset de teste
Parquet/CSV	Armazenamento otimizado
Logging	Registro de execuÃ§Ã£o e erros

ğŸ“ˆ Resultados Esperados
ApÃ³s a execuÃ§Ã£o:

O diretÃ³rio data/processed/ conterÃ¡ os arquivos limpos e rejeitados.

O log completo estarÃ¡ em logs/pipeline_normalizacao_avancado.log.

ğŸ§‘â€ğŸ’» Autor
Sthefany Spina
ğŸ’¡ Projeto educativo e demonstrativo de Data Engineering com PySpark
ğŸ“¬ Sinta-se livre para clonar, melhorar e compartilhar!