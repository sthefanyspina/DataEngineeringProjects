# ğŸ§  AnÃ¡lise AvanÃ§ada de Logs Web com PySpark

Projeto de **Data Engineering** utilizando **PySpark** para processar e analisar **logs de servidores web (Apache/Nginx)**.  
O objetivo Ã© extrair informaÃ§Ãµes Ãºteis, como IPs, URLs acessadas, horÃ¡rios e cÃ³digos de status, gerando mÃ©tricas de uso e erros.

---

## ğŸ“‹ Objetivos

- Ler arquivos de log de acesso (`access.log`)
- Extrair campos relevantes:
  - IP de origem  
  - Data e hora do acesso  
  - MÃ©todo HTTP (`GET`, `POST`, etc.)  
  - URL acessada  
  - CÃ³digo de status (`200`, `404`, `500`...)  
  - Tamanho da resposta  
- Calcular estatÃ­sticas:
  - ğŸ“Š Acessos por hora  
  - ğŸŒ PÃ¡ginas mais acessadas  
  - ğŸ‘¥ IPs mais ativos  
  - âŒ Erros HTTP (404, 500 etc.)  
  - ğŸ“ˆ Taxa de erro (%)

---

## âš™ï¸ Tecnologias Utilizadas

| Tecnologia | DescriÃ§Ã£o |
|-------------|------------|
| **Python 3.9+** | Linguagem principal |
| **Apache Spark (PySpark)** | Engine de processamento distribuÃ­do |
| **Parquet** | Formato de saÃ­da otimizado |
| **Regex (expressÃµes regulares)** | ExtraÃ§Ã£o de dados do log |
| **Logging** | Registro de execuÃ§Ã£o e erros |

---

## ğŸ“‚ Estrutura de Pastas

pyspark-analise-logs-web/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ logs/
â”‚ â”‚ â”œâ”€â”€ access_log_2025-10-10.log
â”‚ â”‚ â”œâ”€â”€ access_log_2025-10-11.log
â”‚ â””â”€â”€ output/
â”‚ â”œâ”€â”€ acessos_por_hora/
â”‚ â”œâ”€â”€ paginas_populares/
â”‚ â”œâ”€â”€ ips_ativos/
â”‚ â”œâ”€â”€ erros_http/
â”‚
â”œâ”€â”€ logs/
â”‚ â””â”€â”€ analise_logs_web_avancada.log
â”‚
â”œâ”€â”€ analise_logs_web_avancada.py
â””â”€â”€ README.md

yaml
Copy code

---

## ğŸ§© Exemplo de Linha de Log (Apache/Nginx)

192.168.1.10 - - [12/Oct/2025:10:45:22 -0300] "GET /produtos HTTP/1.1" 200 5321

yaml
Copy code

---

## ğŸš€ ExecuÃ§Ã£o do Projeto

### 1ï¸âƒ£ Criar o ambiente
Crie e ative um ambiente virtual Python:
```bash
python -m venv venv
source venv/bin/activate   # Linux / Mac
venv\Scripts\activate      # Windows
2ï¸âƒ£ Instalar dependÃªncias
bash
Copy code
pip install pyspark
3ï¸âƒ£ Estrutura de dados
Coloque seus arquivos de log em:

bash
Copy code
/data/logs/
4ï¸âƒ£ Executar o script
bash
Copy code
spark-submit analise_logs_web_avancada.py
ğŸ“Š SaÃ­das Geradas
Os resultados sÃ£o salvos em formato Parquet na pasta /data/output/:

MÃ©trica	Caminho	DescriÃ§Ã£o
Acessos por Hora	/data/output/acessos_por_hora/	Total de acessos por hora
PÃ¡ginas Populares	/data/output/paginas_populares/	URLs mais acessadas
IPs Ativos	/data/output/ips_ativos/	IPs que mais acessaram o site
Erros HTTP	/data/output/erros_http/	Contagem de cÃ³digos 4xx e 5xx

ğŸ“ˆ Exemplo de SaÃ­da
Acessos por Hora
hora	count
0	154
1	201
2	189

PÃ¡ginas mais Acessadas
url	count
/home	1289
/produtos	912
/carrinho	405

Erros HTTP
status	count
404	38
500	5

Taxa de erro: 3.1%

ğŸ› ï¸ Como o CÃ³digo Funciona
LÃª os logs em formato texto (spark.read.text())

Extrai campos com expressÃµes regulares (regexp_extract)

Converte o timestamp (to_timestamp)

Calcula mÃ©tricas com groupBy e count

Salva resultados em formato Parquet

Gera logs de execuÃ§Ã£o (/logs/analise_logs_web_avancada.log)

ğŸ“¦ PrÃ³ximos Passos
 Adicionar Delta Lake para histÃ³rico incremental

 Criar pipeline no Airflow para execuÃ§Ã£o diÃ¡ria

 Enviar alertas em caso de alta taxa de erro

 Visualizar mÃ©tricas em Power BI / Grafana

ğŸ§‘â€ğŸ’» Autor
Sthefany Spina
ğŸ’¬ Projeto educacional com foco em Engenharia de Dados e PySpark.