# ğŸŸ¢ Pipeline de IngestÃ£o Batch com PySpark

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Spark](https://img.shields.io/badge/Apache%20Spark-3.x-orange)
![Airflow](https://img.shields.io/badge/Airflow-2.x-lightgrey)
![License](https://img.shields.io/badge/License-MIT-green)

## ğŸš€ DescriÃ§Ã£o

Pipeline de ingestÃ£o batch utilizando **PySpark**, com validaÃ§Ã£o automÃ¡tica de dados (**Great Expectations**), armazenamento em **Data Lake (S3/GCS/HDFS)**, geraÃ§Ã£o de **relatÃ³rios de erros** e carga em **banco SQL**. Ideal para ambientes de produÃ§Ã£o e testes com grandes volumes de dados.

---

## ğŸ“‚ Estrutura do Projeto

pyspark_pipeline_batch/
â”‚
â”œâ”€â”€ dags/
â”‚ â””â”€â”€ ingestao_batch.py # DAG do Airflow
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ input/ # Arquivos CSV e JSON de entrada
â”‚ â””â”€â”€ rejected/ # Registros rejeitados
â”œâ”€â”€ logs/
â”‚ â””â”€â”€ pipeline.log # Logs do pipeline
â”œâ”€â”€ great_expectations/ # ValidaÃ§Ãµes automÃ¡ticas
â”œâ”€â”€ main_batch_pipeline.py # Script principal do pipeline
â””â”€â”€ requirements.txt # DependÃªncias do projeto

yaml
Copy code

---

## âš™ï¸ PrÃ©-requisitos

- Python 3.8+
- Java 8+ (para Spark)
- Apache Spark 3.x
- Apache Airflow 2.x (opcional)
- Bibliotecas Python:

```bash
pip install pyspark pandas great_expectations psycopg2-binary
Banco de dados PostgreSQL ou outro compatÃ­vel com JDBC

Acesso a Data Lake (S3 ou GCS)

ğŸ—„ï¸ ConfiguraÃ§Ã£o do Banco SQL
Exemplo PostgreSQL:

sql
Copy code
CREATE TABLE clientes (
    id INT PRIMARY KEY,
    nome VARCHAR(100),
    idade INT,
    renda DECIMAL
);

CREATE TABLE pedidos (
    pedido_id INT PRIMARY KEY,
    cliente_id INT,
    valor DECIMAL,
    data DATE
);
Configurar credenciais no main_batch_pipeline.py:

python
Copy code
jdbc_url = "jdbc:postgresql://localhost:5432/empresa"
db_properties = {
    "user": "postgres",
    "password": "senha123",
    "driver": "org.postgresql.Driver"
}
ğŸƒ ExecuÃ§Ã£o do Pipeline
Manual
bash
Copy code
python main_batch_pipeline.py
Via Airflow
Copie a DAG dags/ingestao_batch.py para o diretÃ³rio de DAGs do Airflow.

Inicie o scheduler e webserver:

bash
Copy code
airflow scheduler
airflow webserver
A DAG serÃ¡ executada diariamente conforme o agendamento configurado.

âœ… Funcionalidades
Leitura de CSV e JSON da pasta data/input/.

ValidaÃ§Ã£o automÃ¡tica de dados usando Great Expectations:

Campos obrigatÃ³rios nÃ£o nulos

Tipos de dados corretos

Registros rejeitados:

CSV: data/rejected/*.csv

HTML: data/rejected/*.html

Armazenamento no Data Lake:

Ex.: s3a://meu-data-lake/bronze/clientes/

Carga em banco SQL (PostgreSQL, MySQL ou SQL Server)

Logs detalhados em logs/pipeline.log.

ğŸ“Š Dados de Teste
clientes.csv â†’ +200.000 linhas

pedidos.json â†’ +200.000 registros

Exemplo:

csv
Copy code
# clientes.csv
id,nome,idade,renda
1,Cliente_1,25,3500.50
2,Cliente_2,40,4200.00
json
Copy code
# pedidos.json
{"pedido_id":1,"cliente_id":1,"valor":150.75,"data":"2025-01-12"}
{"pedido_id":2,"cliente_id":2,"valor":320.40,"data":"2025-03-15"}