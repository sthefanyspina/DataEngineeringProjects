Data Lake Pipeline com PySpark â€“ Bronze, Silver e Gold
ğŸ“– DescriÃ§Ã£o

Este projeto implementa um Data Lake em camadas utilizando PySpark. Ele simula um fluxo completo de ingestÃ£o, limpeza, deduplicaÃ§Ã£o e enriquecimento de dados de vendas, seguindo a arquitetura Bronze â†’ Silver â†’ Gold:

Bronze: dados brutos (raw)

Silver: dados tratados e deduplicados

Gold: dados agregados e prontos para anÃ¡lise

O pipeline Ã© totalmente automatizado e registra logs para rastreabilidade.

ğŸ—‚ Estrutura do Projeto
data_lake_pipeline/
â”‚
â”œâ”€â”€ input/
â”‚   â””â”€â”€ vendas_200000.csv       # arquivo CSV com dados de vendas
â”‚
â”œâ”€â”€ data_lake/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ pipeline_datalake_YYYYMMDD_HHMMSS.log
â”‚
â”œâ”€â”€ pipeline_datalake.py        # script PySpark do pipeline
â””â”€â”€ README.md

âš™ï¸ PrÃ©-requisitos

Python 3.8+

Apache Spark 3.x

Pandas (para geraÃ§Ã£o do dataset de teste)

Sistema operacional com capacidade para Spark (Windows, Linux, macOS)

ğŸ“ InstalaÃ§Ã£o e Setup

Clone o repositÃ³rio:

git clone https://github.com/seu-usuario/data_lake_pipeline.git
cd data_lake_pipeline


Instale dependÃªncias Python (opcional, apenas para geraÃ§Ã£o do CSV de teste):

pip install pandas numpy


Crie a pasta input e gere o dataset de teste:

# Execute este script Python
import pandas as pd
import numpy as np

np.random.seed(42)
n = 200_000
id_venda = np.arange(1, n + 1)
nomes = ['Ana','Carlos','Beatriz','JoÃ£o','Marcos','Julia','Paula','Rafael','Fernanda','Lucas']
cliente = np.random.choice(nomes, size=n)
valor_venda = np.random.uniform(10,1000,size=n).round(2)
df = pd.DataFrame({'id_venda': id_venda, 'cliente': cliente, 'valor_venda': valor_venda})
df.to_csv("input/vendas_200000.csv", index=False)

ğŸš€ Executando o Pipeline

Certifique-se de ter Spark instalado e configurado (spark-submit disponÃ­vel).

Execute o pipeline:

spark-submit pipeline_datalake.py


Ao final da execuÃ§Ã£o, os dados serÃ£o salvos nas camadas:

data_lake/bronze/vendas_raw.parquet

data_lake/silver/vendas_clean.parquet

data_lake/gold/vendas_aggregated.parquet

Logs de execuÃ§Ã£o estarÃ£o em logs/.

ğŸ”§ Funcionalidades do Pipeline

IngestÃ£o (Bronze): lÃª CSV bruto e salva em Parquet

Limpeza e deduplicaÃ§Ã£o (Silver): padroniza texto, converte tipos e remove duplicados

AgregaÃ§Ã£o e enriquecimento (Gold): gera mÃ©tricas como total de vendas e quantidade de compras por cliente

Logs: registra todas as etapas com contagem de linhas e erros

ğŸ“Š Exemplo de Resultado (Camada Gold)
cliente	total_vendas	qtd_compras
ana	200.0	2
carlos	150.0	1
beatriz	300.0	3