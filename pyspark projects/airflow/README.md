ğŸ­ Airflow + PySpark Data Lake Pipeline










Pipeline diÃ¡rio de dados que utiliza Apache Airflow e PySpark, seguindo a arquitetura Bronze â†’ Silver â†’ Gold, baixando dados de uma API pÃºblica (COVID-19), transformando e salvando em um Data Lake local.

ğŸ“‚ Estrutura do Projeto
airflow_spark/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ pyspark_datalake_dag.py       # DAG do Airflow
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ process_datalake.py           # Script PySpark
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze/                       # Dados brutos (CSV)
â”‚   â”œâ”€â”€ silver/                       # Dados tratados (Parquet)
â”‚   â””â”€â”€ gold/                         # Dados agregados (Parquet)
â”‚
â”œâ”€â”€ docker-compose.yml                 # Containers Airflow + Spark
â””â”€â”€ requirements.txt                   # DependÃªncias Python

âš™ï¸ PrÃ©-requisitos

Docker & Docker Compose

Internet para baixar dados da API

Opcional: Para salvar no S3 ou GCS, configure as credenciais correspondentes e altere os caminhos no script PySpark.

ğŸš€ Como rodar o projeto

Clone o repositÃ³rio:

git clone https://github.com/seu-usuario/airflow_spark.git
cd airflow_spark


Suba os containers:

docker-compose up -d


Acesse o Airflow Web UI:

http://localhost:8080


Ative a DAG pyspark_datalake_dag.
VocÃª pode executar manualmente clicando em Trigger DAG ou aguardar a execuÃ§Ã£o diÃ¡ria automÃ¡tica.

ğŸ§© Fluxo do Pipeline
Etapa	DescriÃ§Ã£o	Camada
IngestÃ£o	Baixa dados da API COVID-19	Bronze
TransformaÃ§Ã£o	Limpeza e normalizaÃ§Ã£o	Silver
AgregaÃ§Ã£o	KPIs e resumos por paÃ­s	Gold
Agendamento	Rodado diariamente pelo Airflow	AutomÃ¡tico
ğŸ“Œ Tecnologias

Apache Airflow: OrquestraÃ§Ã£o de workflows

PySpark: Processamento distribuÃ­do de dados

Docker: ContÃªinerizaÃ§Ã£o para execuÃ§Ã£o isolada

Pandas: Leitura inicial da API

ğŸ—„ï¸ Data Lake Local

data/bronze/ â†’ CSV bruto

data/silver/ â†’ Parquet tratado

data/gold/ â†’ Parquet agregado

Para nuvem, altere os caminhos no script PySpark para S3 ou GCS.

ğŸ”§ CustomizaÃ§Ãµes

Alterar a API de origem no script process_datalake.py

Alterar a frequÃªncia de execuÃ§Ã£o no DAG (schedule_interval)

Adicionar novas transformaÃ§Ãµes ou agregaÃ§Ãµes no script PySpark

ğŸ’¡ ObservaÃ§Ãµes

Pipeline extensÃ­vel para qualquer fonte de dados e transformaÃ§Ãµes

AdaptÃ¡vel para produÃ§Ã£o, salvando dados diretamente em AWS S3, GCP Storage ou HDFS

ğŸ“ˆ VisualizaÃ§Ã£o do Pipeline
[API COVID-19] â†’ [Bronze CSV] â†’ [Silver Parquet] â†’ [Gold Parquet]
        â”‚                 â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DAG Airflow â”€â”€â”€â”€â”€â”€â”€â”€â”˜