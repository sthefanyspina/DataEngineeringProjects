# ğŸ§¹ Pipeline de NormalizaÃ§Ã£o de Dataset (Data Cleaning Project)

Este projeto implementa um **pipeline automatizado de limpeza e normalizaÃ§Ã£o de dados** em Python.  
Ele foi criado para processar datasets desorganizados â€” com colunas inconsistentes, valores ausentes, tipos errados e duplicatas â€” e gerar uma versÃ£o limpa, pronta para anÃ¡lise ou carga em banco de dados.

---

## ğŸš€ Objetivo

O objetivo deste pipeline Ã©:
- Corrigir **nomes de colunas** e **formataÃ§Ãµes inconsistentes**  
- Padronizar **datas, textos e valores numÃ©ricos**  
- Remover **duplicatas e registros invÃ¡lidos**  
- Gerar logs de erros e auditoria  
- Salvar o resultado limpo em **CSV** e tambÃ©m em um **banco de dados**

---

## ğŸ§  Tecnologias Utilizadas

- **Python 3.9+**
- **Pandas** â†’ manipulaÃ§Ã£o e limpeza de dados  
- **NumPy** â†’ tratamento de valores nulos e tipos  
- **SQLite** (padrÃ£o, mas compatÃ­vel com PostgreSQL/MySQL)  
- **Regex** â†’ correÃ§Ã£o e padronizaÃ§Ã£o de nomes de colunas  

---

## ğŸ“ Estrutura do Projeto
ğŸ“¦ data-pipeline-normalizacao
â”œâ”€â”€ dados_bagunÃ§ados.csv # Dataset original com 500k+ linhas e erros
â”œâ”€â”€ dados_limpos.csv # Dataset limpo gerado pelo pipeline
â”œâ”€â”€ dados_normalizados.db # Banco SQLite com os dados limpos
â”œâ”€â”€ log_erros.csv # Log com erros detectados e valores problemÃ¡ticos
â”œâ”€â”€ gerar_dataset.py # Script para gerar dataset sintÃ©tico (bagunÃ§ado)
â”œâ”€â”€ pipeline_normalizacao.py # Pipeline principal de limpeza e normalizaÃ§Ã£o
â””â”€â”€ README.md # Este arquivo

## ğŸ§© Funcionalidades Principais

| Funcionalidade | DescriÃ§Ã£o |
|----------------|------------|
| ğŸ§¾ CorreÃ§Ã£o de nomes de colunas | Remove espaÃ§os e padroniza nomes para o schema final |
| ğŸ§ NormalizaÃ§Ã£o de texto | Remove espaÃ§os extras e padroniza capitalizaÃ§Ã£o |
| ğŸ“… NormalizaÃ§Ã£o de datas | Converte mÃºltiplos formatos para `YYYY-MM-DD`, logando invÃ¡lidas |
| ğŸ’° NormalizaÃ§Ã£o de nÃºmeros | Corrige separadores decimais, converte strings para `float` |
| ğŸ§¹ Limpeza de duplicatas/nulos | Remove registros redundantes ou completamente vazios |
| ğŸªµ Log de erros detalhado | Cria arquivo `log_erros.csv` com linha, coluna, valor e tipo de erro |
| ğŸ—„ï¸ IntegraÃ§Ã£o com banco | Carrega dados limpos em banco SQLite (ou PostgreSQL/MySQL) |

---

## ğŸ§° Como Executar

### 1ï¸âƒ£ Criar o dataset â€œbagunÃ§adoâ€
Execute o script de geraÃ§Ã£o:

```bash
python gerar_dataset.py
Isso criarÃ¡ um arquivo dados_bagunÃ§ados.csv com mais de 500.000 linhas e diversos erros intencionais.

2ï¸âƒ£ Rodar o pipeline de normalizaÃ§Ã£o
bash
python pipeline_normalizacao.py

O script:
LerÃ¡ dados_bagunÃ§ados.csv
CorrigirÃ¡ inconsistÃªncias
GerarÃ¡ dados_limpos.csv
CriarÃ¡ o log log_erros.csv
E salvarÃ¡ os dados no banco dados_normalizados.db

ğŸ§± IntegraÃ§Ã£o com Banco de Dados
Por padrÃ£o, o pipeline usa SQLite, mas vocÃª pode facilmente trocar por outro banco:

Exemplo para PostgreSQL
python
Copy code
from sqlalchemy import create_engine
engine = create_engine("postgresql+psycopg2://usuario:senha@host:porta/banco")
df.to_sql("clientes", engine, if_exists="replace", index=False)

Exemplo para MySQL
python
Copy code
engine = create_engine("mysql+pymysql://usuario:senha@host:porta/banco")

ğŸ§¾ Log de Erros
O arquivo log_erros.csv contÃ©m todas as linhas que apresentaram falhas de conversÃ£o:

linha	coluna	valor	erro
1234	Data_nasc	31/02/2020	Data invÃ¡lida
9876	SalÃ¡rio	mil	NÃºmero invÃ¡lido

Esses registros podem ser revisados manualmente ou descartados conforme a polÃ­tica de qualidade dos dados.

âš™ï¸ PersonalizaÃ§Ãµes
VocÃª pode ajustar:
Mapeamento de colunas â†’ no dicionÃ¡rio dentro da funÃ§Ã£o corrigir_nomes_colunas
Tratamentos adicionais â†’ ex: padronizaÃ§Ã£o de UF, CEP, gÃªnero etc.
Banco de destino â†’ trocando a string de conexÃ£o na funÃ§Ã£o principal

ğŸ‘©â€ğŸ’» Autor
Sthefany Spina
Projeto educativo de Engenharia de Dados â€¢ Pipeline de NormalizaÃ§Ã£o de Datasets
